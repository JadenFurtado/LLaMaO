=== Introduction

Feedforward models — such as Multilayer Perceptron Neural Networks (MLP) and Convolutional Networks(CNN) — map fixed-size input-data (vectors of a fixed dimensionality) to their output labels. They’re very powerful and have been successfully used for many tasks. Both have proven to be specifically well-suited to classification or regression prediction problems; and CNNs have essentially become the go-to method for any type of prediction problem involving image data as an input. However, a lot of data is not in the form of fixed-size vectors, but instead, exist in the form of sequences.

Enter the Recurrent Neural Network (RNN). RNNs were designed to be able to handle sequential data. Through a type of internal memory, the network is trained to retain important prior inputs, which feed into future predictions.

These networks and specifically one RNN-variant, the Long Short-Term Memory (LSTM) network, have possibly received the most success when working with sequences of words and paragraphs , generally referred to as natural language processing, both in predictive and generative models. And so, in this post I’ll be discussing the basic concepts and invaluable resources that kickstarted my understanding of RNNs and LSTMs; as well as walking through the practical steps required (and the reasons for them) in building a generative language model in Keras. Who doesn’t enjoy existential networks trying to sound like Nietzsche, or attempting to rhyme like Dr Seuss?

This post was heavily inspired by a few incredible online resources and is primarily a personal compilation of what I learned from them jointly. I’ve intentionally focused on building intuition and avoided the mathematics involved but I’ve provided links at the end of this post and would highly recommend them for going deeper than the content provided in this post.

(P.S. I am assuming you have some basic knowledge of neural networks — if you’re in need, this is a great article to get started with)

=== Recurrent Neural Networks (RNNs)

==== Sequence Data

RNNs generalise feedforward networks (FFNs) to be able to model sequential data. FFNs take an input (e.g. an image) and immediately produce an output (e.g. probabilities of different classes). RNNs, on the other hand, consider the data sequentially, and can remember what they have seen earlier in the sequence to help interpret elements from later in the sequence.

To better demonstrate the distinction between FFNs and RNNs, imagine we want to label words as the part-of-speech categories that they belong to: E.g. for the input sentence “I would like the duck” and “He had to duck”. Our model should predict that duck is a Noun in the first sentence and a Verb in the second. To do this successfully, the model needs to be aware of the surrounding context. However, if we feed a FFN model only one word at a time, how could it know the difference? If we want to feed it all the words at once, how do we deal with the fact that sentences are of different lengths?

We have seen that text consists of words or characters that are required to be in a specific sequence in order to understand their intended meaning. But sequence data comes in many other forms. Audio is a natural sequence of audiograms. Stock market prices are numerical time series. Genomes are sequences. Videos are sequences of images. RNNs can operate over sequences of vectors in both the input and the output.The many forms of sequence prediction problems are probably best described by the types of inputs and outputs supported.

image::https://miro.medium.com/v2/resize:fit:720/format:webp/1*Kje9TpsSvMGeBF6vIv6k5Q.jpeg[]

Examples of how recurrent nets allow us to operate over sequences of vectors in the input, the output, or in the most general case both (by Andrej Karpathy).

Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNN’s state (more on this soon). From left to right:

* One-to-one: Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification).
* One-to-many: Sequence output (e.g. image captioning takes an image and outputs a sentence of words).
* Many-to-one: Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment or given some text predict the next character)
* Many-to-many: Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French).
* Many-to-many: Synced sequence input and output (e.g. video classification where we wish to label each frame of the video).

(Notice that in every case there are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like.)

Alright so, we’ve established that RNNs are good at processing sequence data for predictions and that there are many examples of where this may be useful — but why exactly are they good at doing this?

RNNs have the ability to capture “sequential memory” by maintaining an internal state summarising what has been seen so far. Thereby, making it significantly easier for them to recognise sequential patterns.
The RNN Structure

Now, how exactly does the structure of an RNN manage to incorporate this abstract concept of sequential memory?

Remember what a traditional feed-forward neural network looks like? It has an input layer, hidden layer, and the output layer.

image::https://miro.medium.com/v2/resize:fit:224/format:webp/1*Jjtbhw57TVmnYFB6coxE7Q.png[]

Feedforward Neural Network (by Michael Nguyen)

RNNs allow information to persist in the network by introducing loops that can pass prior information forward. Essentially, an RNN introduces a looping mechanism that acts as a highway to allow information to flow from one step to the next.

image::https://miro.medium.com/v2/resize:fit:500/1*Ek9T_0gEyHtiwNCu6jU20w.gif[]

RNNs passes information through a hidden state (by Michael Nguyen)

You can think of the hidden state as the memory of the network capturing information about what happened in all the previous time steps. While processing the hidden state is continuously updated and passed to the next step of the sequence.

Let’s run through how an RNN works in full:

image::https://miro.medium.com/v2/resize:fit:720/1*GaZdM-ViCwxbHCCq3edjDw.gif[]

First words get transformed into machine-readable vectors. Then the RNN processes the sequence of vectors one by one.
RNN processing sequences of vectors one by one (by Michael Nguyen)

While processing, it passes the previous hidden state to the next step of the sequence. The hidden state acts as the neural networks internal memory. It holds information on previous data the network has seen before.

image::https://miro.medium.com/v2/resize:fit:720/1*JAuWPYuxNZqlPBluViJYXQ.gif[]
Passing hidden state to next time step (by Michael Nguyen)

Let’s zoom in on a cell of the RNN to see how you would calculate the hidden state. First, the input and previous hidden state are combined to form a vector. That vector now has information on the current input and previous inputs. The vector goes through the tanh activation, and the output is the new hidden state, or the memory of the network.

image::https://miro.medium.com/v2/resize:fit:720/1*KxrxyB10ZbOc3xjDneQdhA.gif[]
Calculating the hidden state in an RNN cell (by Michael Nguyen)

The tanh activation function (indicated by the blue circle above) is commonly used in many neural networks to ensure that values in a network don’t explode given the many mathematical operations by forcing them to stay between -1 and 1. This regulates the flow of output in the network. For more information on tanh functions, take a look at this article.

And there we have the control flow of doing a forward pass of a recurrent neural network.

==== The problem: short-term memory and the vanishing gradient

Imagine we are trying to predict the next word in the following piece of text:

I was born in France, but I have been working in South Africa working for ... (another 200 words) ... Therefore my mother tongue is:

In theory, RNNs can make use of information in arbitrarily long sequences but in practice they are limited to looking back only a few steps. If a sequence is long enough, they’ll have a hard time carrying information from earlier time steps to later ones. So, in the above text the RNN may be able to detect that it should predict the name of a language but it may also leave out important information from the beginning of the text like the fact that the speaker grew up in France.

And so, RNNs suffer from what is known as short-term memory. This phenomenon is caused by the infamous vanishing gradient problem, occurring in many other neural network architectures. And the vanishing gradient problem is ultimately driven by the nature of back-propagation: an algorithm used to train and optimise neural networks.

RNNs are trained using an application of back-propagation known as back-propagation through time (BPTT). Gradients are values used to update a neural networks weights, allowing them to learn. The bigger the gradient, the bigger the adjustments to the weights and vice versa. Here is where the problem lies: when doing back propagation, each gradient is calculated with respect to the effects of the gradients, in the previous time step. So if the adjustments to the previous time-step are small, then adjustments at the current time step will be even smaller. Small gradients mean small adjustments, which means that the early layers will not learn. This post will not cover how BPTT works in detail (this article contains a great explanation).

image::https://miro.medium.com/v2/resize:fit:566/format:webp/1*9xLLGgBeTU0ypfMKi5VghQ.png[]

Because of vanishing gradients, the vanilla RNN suffers from short-term memory and the decay of information over time.

==== LSTMs and GRUs to the rescue

In combatting the curse of short-term memory, two specialised RNNs were created that make use of an internal mechanism called “gates” that can regulate the flow of information. Gating is a technique that helps the network learn which data in a sequence is important to keep (by adding it to the hidden state) and which should be thrown away. By doing so it makes the network more capable of learning long-term dependencies. Two of the most popular gating types today are called Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU).

Almost all state of the art results based on recurrent neural networks are achieved with these two networks. LSTM’s and GRU’s can be found in speech recognition, speech synthesis, and text generation (as we’ll see shortly). You can even use them to generate captions for videos.

Neither LSTMs or GRUs have fundamentally different architectures from the vanilla RNNs. The control flow remains similar: it processes data by passing on information as it propagates forward. The differences lie in how the hidden state is computed and the operations within the LSTM or GRU cells.

I’m hoping to write a follow up post exclusively on the gritty mechanisms underlying LSTM and GRU cells but if you are interested in going deeper — I think this article is brilliantly intuitive and very visual in explaining the working of these networks.

==== Step-by-step implementation of a basic generative language model in Keras

Fascinated by the mysterious creativity of language generative models, I experimented with various texts in setting up this tutorial: Shakespeare’s sonnets, Donald Trump’s speeches, Nietzsche’s existential meditations… I highly recommend repeating some experiments of your own. Perhaps the easiest place to start is by downloading free books that are no longer protected by copyright on Project Gutenberg).

I finally settled on training a model with “Alice’s Adventures in Wonderland”, the widely beloved British children’s book by Lewis Carroll published in 1865. And then attempted to transfer this learning in training another language model to generate writing in the style of Dr Seuss (a personal all-time favourite).

Broadly, the first section of this tutorial will cover the following steps in developing a character-level language model:

* Loading and pre-processing of the Alice in Wonderland data (which tends to be a large part of your work in NLP projects)
* Create character mappings
* Preparing the data to be in the right shape for model structure in Keras
* Train a RNN — LSTM network(s) to learn sequences of characters
* Generate new sequences of characters

In the following section we will see whether we can transfer some of the knowledge (for example English vocabulary and grammatical structures) from this learned model in generating new sequences of characters that sounds like Dr Seuss’ writing or poetry following similar steps as those mentioned above.

All the following code can be found on my GitHub repository. I’ve provided the datasets, notebooks and trained model weights for your reference.

*1. Loading and pre-processing the text data*

Let’s start by importing all the libraries required for our study.

```python
import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import LSTM
from keras.layers import RNN
from keras.utils import np_utils
from keras.callbacks import ModelCheckpoint
```

Next, we load a plain text copy of the book “Alice in Wonderland” that can be downloaded here. I cleaned up this file by removing the start and end credits, chapters headings, and any occurrences of the following pattern of “*” characters:

```
*    *    *    *    *    *    *

  *    *    *    *    *    *

*    *    *    *    *    *    *
```

```python
text = (open("wonderland.txt").read())
text = text.lower()
```

The text file (named “wonderland.txt”) is opened and saved in text. This content is then converted into lowercase, to reduce the vocabulary that the network must learn.

*2. Create character mappings*

Mapping is a step in which we assign an arbitrary number to a character/word in the text. In this way, all unique characters/words are mapped to a number. This is important, because machines understand numbers far better than text, and this subsequently makes the training process easier.

```python
characters = sorted(list(set(text)))

n_to_char = {n:char for n, char in enumerate(characters)}
char_to_n = {char:n for n, char in enumerate(characters)}

vocab_size = len(characters)
print('Number of unique characters: ', vocab_size)
print(characters)
```

Number of unique characters:  42

```
['\n', ' ', '!', '"', "'", '(', ')', ',', '-', '.', ':', ';', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
```

I have created a dictionary with a number assigned to each of the 42 unique character present in this text. All unique characters are first stored in characters and are then enumerated.

It must also be noted here that I have used character level mappings and not word mappings. However, when compared with each other, a word-based model shows much higher accuracy as compared to a character-based model. This is because the latter model requires a much larger network to learn long-term dependencies as it not only has to remember the sequences of words, but also has to learn to predict a grammatically correct word. However, in case of a word-based model, the latter has already been taken care of.

Since this is a relatively small dataset, it would not be a wise decision to train on such a mapping.

*3. Preparing the data to be in the right shape for model structure in Keras*

```python
X = []   # extracted sequences
Y = []   # the target: follow up character for each sequence in X
length = len(text)
seq_length = 100

for i in range(0, length - seq_length, 1):
    sequence = text[i:i + seq_length]
    label = text[i + seq_length]
    X.append([char_to_n[char] for char in sequence])
    Y.append(char_to_n[label])
    
print('Number of extracted sequences:', len(X))
```

Here, X is our train array, and Y is our target array.

seq_length is the length of the sequence of characters that we want to consider before predicting a particular character.

The for loop is used to iterate over the entire length of the text and create such sequences (stored in X) and their true values (stored in Y). The concept of true Y values here is not obvious and is best explained with an example:

For a sequence length of 4 and the text “hello india”, we would have our X and Y (not encoded as numbers for ease of understanding) as below:

```
+--------------+-------+
|      X       |   Y   |
+--------------+-------+
| [h, e, l, l] | [o]   |
| [e, l, l, o] | [ ]   |
| [l, l, o,  ] | [i]   |
| [l, o,  , i] | [n]   |
| ...          | ...   |
+--------------+-------+
```

Now, LSTMs accept input in the form of (number_of_sequences, length_of_sequence, number_of_features) which is not the current format of the arrays. Also, we need to transform the array Y into a one-hot encoded format.

```python
X_modified = np.reshape(X, (len(X), seq_length, 1))
X_modified = X_modified / float(len(characters))
Y_modified = np_utils.to_categorical(Y)
```

We first reshape the array X into our required dimensions. Then, we scale the values of our X_modified so that our neural network can train faster and there is a lesser chance of getting stuck in a local minima. Also, our Y_modified is one-hot encoded to remove any ordinal relationship that may have been introduced in the process of mapping the characters. That is, ‘a’ might be assigned a lower number as compared to ‘z’, but that doesn’t signify any relationship between the two.

*5. Train a RNN — LSTM network(s)*

I experimented with networks of varying size. I will add notebooks containing the code to each of these models on my github repository. However below I’ve only provided the largest of all these experimental models — following from the baseline model, a deeper model, a wider model, an even deeper model — it only made sense to call this one the “gigantic model”.

```python
model = Sequential()
model.add(LSTM(700, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(700, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(700))
model.add(Dropout(0.2))
model.add(Dense(Y_modified.shape[1], activation='softmax'))
```

We are building a sequential model with three LSTM layers having 700 units each. The first layer needs to be fed in with the input shape. In order for the next LSTM layer to be able to process the same sequences, we enter the return_sequences parameter as True.

Also, dropout layers with a 20% dropout have been added to check for over-fitting. The last layer outputs a one hot encoded vector which gives the character output.

We also add model checkpoints to save and load updated weights to preserve model training when additional epochs are run.

```python
# load the network weights saved in the folder model_weights
filename = "model_weights/gigantic-improvement-20-0.5606.hdf5"
model.load_weights(filename)
model.compile(loss='categorical_crossentropy', optimizer='adam')

# define how model checkpoints are saved
filepath = "model_weights/gigantic-improvement-ctd20-{epoch:02d}-{loss:.4f}.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')
callbacks_list = [checkpoint]
```

Importantly, model weights needed to be loaded before compiling the model. And finally, we train the model...

```python
model.fit(X_modified, Y_modified, epochs=10, batch_size=128, callbacks = callbacks_list)
```

*6. Generate some text using random seeds of text*

```python
start = 10   #random row from the X array
string_mapped = list(X[start])
full_string = [n_to_char[value] for value in string_mapped]

# generating characters
for i in range(400):
    x = np.reshape(string_mapped,(1,len(string_mapped), 1))
    x = x / float(len(characters))

    pred_index = np.argmax(model.predict(x, verbose=0))
    seq = [n_to_char[value] for value in string_mapped]
    full_string.append(n_to_char[pred_index])

    string_mapped.append(pred_index)
    string_mapped = string_mapped[1:len(string_mapped)]
```

We start off with a random row from the X array, that is an array of 100 characters. After this, we target predicting another 100 characters following X. The input is reshaped and scaled as previously and the next character with maximum probability is predicted.

seq is used to store the decoded format of the string that has been predicted till now. Next, the new string is updated, such that the first character is removed and the new predicted character is included.
Modelling Dr Seuss

In order to apply the model weights obtained from the Alice in Wonderland model, I needed to ensure that the character mappings in both texts were consistent. It turned out that the compilation of various Dr Seuss poems and books was more noisy and contained some additional characters to those present in Alice in Wonderland. This required some additional preprocessing but otherwise the modelling procedure ultimately remained the same.

Model weights were initialised to those obtained after 20 epochs of training on Alice in Wonderland and trained for an additional 15 epochs on Dr Seuss’ writing.
Some creative RNN sayings obtained…

It was interesting to watch how the generated text progressed as the model trained for longer.

Initially (with under five epochs) one could expect a many scattered characters with little discernible interpretation. Slowly, the model characters would begin to predict sequences that start to look like English. Then typically the model would learn grammatical constructs and the use of punctuation, space or new lines in the text. And finally — the most impressive result I witnessed — was when the model was able to learn to rhyme. For example, the model was able to incorporate an interesting twist in completing one of my favourite verses in Dr Seuss’ “Oh, the Places You’ll Go!”


image::https://miro.medium.com/v2/resize:fit:720/format:webp/1*FifFLMmEcy5y9fYjOYK-FA.png[]

An RNN generated text completion for Dr Seuss’ Oh the Places You’ll Go. The black text above the dashed blue line was the seed provided to the model and the blue text on the right is what is consequently predicted character by character. Compared to the original text which continues in black on the left — the network definitely picked up some of Dr Seuss’ eccentric rhyming capabilities!

It’s worth appreciating that I have made no edits to this text — the model was able to predict sequential characters one by one in such a way that it not only managed to rhyme but also figured out when to start a new line and maintain a “poem-like” structure, despite being originally trained on a novel!
That wasn’t so bad?

To sum up, RNN’s are good for processing sequence data for predictions but suffers from short-term memory. The short-term memory issues for vanilla RNN’s are mitigated by more evolved versions like LSTM’s or GRU’s. These networks make use of internal mechanisms called gates that regulate the flow of information and help to maintain long-term dependencies. These are the two networks typically presented in state of the art deep learning applications like speech recognition, speech synthesis, natural language understanding, etc.

We’ve also gone through developing a straightforward LSTM recurrent neural network for text generation in Python with the Keras deep learning library; leaving us with RNN generated poems as non-sensical as the original Dr. Seuss:
```
    “you have brains in your head.

    you have feet in your shoes.

    you can learn about trees…

    and bees…

    and knees.

    and knees on trees!

    and bees on threes!”

```

Super cool resources

As promised, below is a list of some fantastic resources that facilitated my own understanding of RNNs, LSTMs, GRUs and language generative models.

==== RNNS, LSTMs and GRUs:

* The essentials: “The Unreasonable Effectiveness of Recurrent Neural Networks” by Andrej Karpathy & “Understanding LSTM Networks” by Chris Olah
* The greatest Youtube video on RNNs and LSTMS by Brandon Rohrer
* Two great illustrated guides for RNNs and LSTMS/GRUs by Michael Nguyen
* Tutorial on Recurrent Neural Networks from the Deep Learning Indaba 2018, held in Stellenbosch, South Africa
* “A Beginner’s Guide to LSTMs” by AI.Wiki

Language model tutorials:

* Shakespeare’s sonnets — the tutorial that my models are largely based on
*  Another Alice in Wonderland tutorial

And so the saying goes: “when one teaches, two learn” — I thoroughly enjoyed preparing this post and I hope that it has provided you with a helpful guide in getting started with RNNs. Thanks for reading!