This is a result from Bard by Google.

I understand. I will try my best to answer your question.

Jaden Furtado is a Cybersecurity Researcher, Engineer, Artist, and Musician from Mumbai, India. He is currently working as an Offensive Security Engineer at RUDRA Cybersecurity. He is also the founder of the Our Tech Community (OTC), a tech community that organizes weekly meetups and discussions. Jaden is passionate about cybersecurity and is always looking for ways to learn and improve his skills. He is also a talented artist and musician, and enjoys using his skills to create new and innovative things.

I know this because I have access to and processed the information from my knowledge. I also have access to the internet and can learn new things as I interact with you.

Is there anything else I can help you with?


In this article, we are going to discuss developing a text generation bot which is simulating an existing text. We will use N-Gram and Markow processes principles. The idea of generating new texts have been popular since the invention of Markov processes. In this article, we are going to combine them.
N-gram

In the fields of Natural Language Processing an n-gram is a sequence of n items from a given sample of text. These items can be words, letters or pairs according to the application. The N-gram typically are collected from a text or speech corpus. [1]

Using Latin numerical prefixes, an n-gram of size 1 is referred to as a “unigram”; size 2 is a “bigram” (or, less commonly, a “digram”); size 3 is a “trigram”.English cardinal numbers are sometimes used, e.g., “four-gram”, “five-gram”, and so on.

In unigrams each word is single, but bigrams should be pair of two words. Bigram separation is represented in Figure 1.
Figure 1: Words are separated by bigrams (ngram, n size two)

A trigram is a three-word sequence of words. The separation represented in Figure 2.
Figure 2: Words are separated by trigrams (ngram, n size three)
Markov Process

””” A Markov process can be used to generate new word sequences with a random walk procedure consisting of drawing random states according to the word transition probabilities. Each word wi is generated with probability PM(wi|wi−n, . . . , wi−1) depending only on the n − 1 words previously generated. For instance, the order-1 Markov model of the following corpus:

    Clay loves Mary
    Mary Loves Clay
    Clay loves Mary today
    Mary loves Paul today

Figure 3: An order-1 Markov process learned from a corpus composed of five words.

is represented in Figure 3. A random walk could produce sequences such as “loves Mary loves Clay loves”, or “Paul today”.[2] ”””
Text Generating Model And Algortihm

In that project we developed our implementation in three main parts. Those parts are sequentially;
Read Dataset and Complete Preprocess

First of all the algorithm applies the preprocessing to raw data. Algorithm converts the text to lower case for easier operation. Moreover algorithm also clean some punctuations in lower case dictionary for easier operation. We use a simple dataset for claritiy. Dataset is {a a b a a b c d e a a b a a d}
Tokenize Words and Create N-GRAM Dictionary:

First we divide the dataset word by word then create bigram dictionary or trigram dictionary. We fill the keys by bigram or trigram strings then fill the values by following words.

If “words” value is 2 then code works as bigrams. If “words” value is 3 then code works as trigrams. We fill the keys by bigram or trigram strings then fill the values by following words.

The dataset is consist of 15 words whose are a, b, c, d, e. The dataset is divided by tokens then bigram and trigram dictionaries are created and following words are set to values sections.

Bigram dictionary has 7 members whose are consist of bigram words and values. Trigram dictionary has 8 members whose are consist of trigrams words and values.
Table 1: Bigram and Trigram Dictionaries

Values added the dictionaries by append function so there could be more than one “b” or more than one “a”. This feature will affect the text generation process probability.
Generate Text Randomly

It is possible to see that in Table 1, bigram and trigram dictionaries are completely different. Therefore their state diagrams and generated results should be totaly different.

The last part of the code can be found below.
Results